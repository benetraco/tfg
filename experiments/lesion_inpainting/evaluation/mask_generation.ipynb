{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1e1122",
   "metadata": {},
   "source": [
    "# Notebook to generate realistic masks on already lesioned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pulp\n",
    "import shutil\n",
    "import ants\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad362a",
   "metadata": {},
   "source": [
    "### Count the subject IDs in the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a1908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_list_middle_numbers(directory, dataset_name):\n",
    "    \"\"\"\n",
    "    Count and list the distinct subject IDs (middle numbers) for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing the .png files.\n",
    "        dataset_name (str): One of ['VH', 'WMH2017', 'SHIFTS'].\n",
    "\n",
    "    Returns:\n",
    "        int: Number of distinct subject IDs.\n",
    "    \"\"\"\n",
    "    middle_numbers = set()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        if dataset_name in ['VH', 'WMH2017'] and filename.startswith(dataset_name + \"_\"):\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                middle_numbers.add(int(parts[1]))\n",
    "\n",
    "        elif dataset_name == 'SHIFTS':\n",
    "            if filename.startswith('train_'):\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    middle_numbers.add(parts[0]+parts[1])\n",
    "            elif filename.startswith(('dev_in', 'eval_in', 'dev_out')):\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 4:\n",
    "                    middle_numbers.add(parts[0]+parts[1]+parts[2])\n",
    "\n",
    "    sorted_ids = sorted(middle_numbers)\n",
    "    print(f\"{dataset_name}: {len(sorted_ids)} distinct subject IDs\")\n",
    "    print(\"Subject IDs:\", ', '.join(map(str, sorted_ids)))\n",
    "    \n",
    "    # return len(sorted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66067065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VH: 18 distinct subject IDs\n",
      "Subject IDs: 648, 727, 728, 729, 738, 739, 741, 743, 744, 745, 746, 747, 749, 751, 752, 754, 755, 758\n",
      "WMH2017: 18 distinct subject IDs\n",
      "Subject IDs: 6, 27, 29, 31, 33, 37, 39, 49, 50, 56, 58, 59, 65, 101, 103, 126, 132, 137\n",
      "SHIFTS: 30 distinct subject IDs\n",
      "Subject IDs: devin2, devin4, devin6, devout14, devout18, devout20, devout21, devout22, devout24, devout25, devout8, evalin10, evalin18, evalin19, evalin21, evalin25, evalin30, evalin33, evalin4, evalin5, evalin9, train14, train19, train22, train23, train28, train3, train32, train4, train6\n",
      "VH: 18 distinct subject IDs\n",
      "Subject IDs: 648, 727, 728, 729, 738, 739, 741, 743, 744, 745, 746, 747, 749, 751, 752, 754, 755, 758\n",
      "WMH2017: 18 distinct subject IDs\n",
      "Subject IDs: 6, 27, 29, 31, 33, 37, 39, 49, 50, 56, 58, 59, 65, 101, 103, 126, 132, 137\n",
      "SHIFTS: 30 distinct subject IDs\n",
      "Subject IDs: devin2, devin4, devin6, devout14, devout18, devout20, devout21, devout22, devout24, devout25, devout8, evalin10, evalin18, evalin19, evalin21, evalin25, evalin30, evalin33, evalin4, evalin5, evalin9, train14, train19, train22, train23, train28, train3, train32, train4, train6\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/flair\"\n",
    "\n",
    "count_and_list_middle_numbers(directory, 'VH')\n",
    "count_and_list_middle_numbers(directory, 'WMH2017')\n",
    "count_and_list_middle_numbers(directory, 'SHIFTS')\n",
    "\n",
    "directory = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\"\n",
    "count_and_list_middle_numbers(directory, 'VH')\n",
    "count_and_list_middle_numbers(directory, 'WMH2017')\n",
    "count_and_list_middle_numbers(directory, 'SHIFTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54000b",
   "metadata": {},
   "source": [
    "### Select and save the test images that will be used with their masks (created)\n",
    "\n",
    "1. Create a table of area of masks for each subject and slice\n",
    "2. Select one image per dataset and slice of different subjects so that the total area is minimal (this way we select the helthier images)\n",
    "3. Save the selected images and their masks in a new folder\n",
    "4. Segment the selected images into WM/GM/CSF\n",
    "5. Create the masks for this new images based on masks from other images in the same slice and that they are not in the CSF/GM areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bded49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_area_table(directory, dataset_name):\n",
    "    \"\"\"\n",
    "    Builds a table with rows as subject IDs, columns as slice numbers, and values as white pixel area.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the folder containing mask .png files.\n",
    "        dataset_name (str): One of ['VH', 'WMH2017', 'SHIFTS'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table of area values per subject and slice.\n",
    "    \"\"\"\n",
    "    table = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        if dataset_name in ['VH', 'WMH2017'] and filename.startswith(dataset_name + \"_\"):\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                subject_id = int(parts[1])\n",
    "                slice_id = int(parts[2].replace(\".png\", \"\"))\n",
    "        elif dataset_name == 'SHIFTS':\n",
    "            parts = filename.split('_')\n",
    "            if filename.startswith('train_') and len(parts) >= 3:\n",
    "                subject_id = parts[0] + '_' + parts[1]\n",
    "                slice_id = int(parts[2].replace(\".png\", \"\"))\n",
    "            elif filename.startswith(('dev_in', 'eval_in', 'dev_out')) and len(parts) >= 4:\n",
    "                subject_id = parts[0] + '_' + parts[1] + '_' + parts[2]\n",
    "                slice_id = int(parts[3].replace(\".png\", \"\"))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Load mask image in grayscale\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None:\n",
    "            print(f\"Warning: could not read image {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Compute white pixel area (assuming white is 255)\n",
    "        # white_area = np.sum(mask == 255)\n",
    "        white_area = np.sum(mask > 0)\n",
    "\n",
    "        # Store in table\n",
    "        if subject_id not in table:\n",
    "            table[subject_id] = {}\n",
    "        table[subject_id][slice_id] = white_area\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame.from_dict(table, orient='index').sort_index(axis=0).sort_index(axis=1)\n",
    "    df.index.name = 'Subject ID'\n",
    "    df.columns.name = 'Slice Number'\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_min_area_combination(df):\n",
    "    \"\"\"\n",
    "    Select one value per column (slice) from different rows (subjects) to minimize total area.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Rows are subjects, columns are slices, values are areas.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (min_area, selected_subjects_dict) where keys are slice IDs and values are subject IDs.\n",
    "    \"\"\"\n",
    "    slice_ids = df.columns.tolist()\n",
    "    subject_ids = df.index.tolist()\n",
    "\n",
    "    # Filter subjects that have all 13 slices\n",
    "    valid_subjects = [s for s in subject_ids if df.loc[s].count() == len(slice_ids)]\n",
    "\n",
    "    if len(valid_subjects) < len(slice_ids):\n",
    "        raise ValueError(\"Not enough subjects with all slices to perform selection.\")\n",
    "\n",
    "    # Create a list of candidate subjects for each slice\n",
    "    candidates = {slice_id: df[slice_id].dropna().sort_values().index.tolist()\n",
    "                  for slice_id in slice_ids}\n",
    "\n",
    "    # Brute-force all combinations of subjects (one per slice)\n",
    "    best_total = float('inf')\n",
    "    best_combo = None\n",
    "\n",
    "    for combo in itertools.permutations(df.index, r=len(slice_ids)):\n",
    "        if len(set(combo)) != len(slice_ids):\n",
    "            continue  # Skip if subject appears more than once\n",
    "\n",
    "        total = 0\n",
    "        valid = True\n",
    "        for i, subject in enumerate(combo):\n",
    "            area = df.iloc[df.index.get_loc(subject), i]\n",
    "            if pd.isna(area):\n",
    "                valid = False\n",
    "                break\n",
    "            total += area\n",
    "\n",
    "        if valid and total < best_total:\n",
    "            best_total = total\n",
    "            best_combo = {slice_ids[i]: combo[i] for i in range(len(slice_ids))}\n",
    "\n",
    "    return best_total, best_combo\n",
    "\n",
    "\n",
    "def solve_min_area_ilp(df):\n",
    "    \"\"\"\n",
    "    Solve the minimal-area mask combination problem using ILP.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Rows = subject IDs, Columns = slice IDs, Values = areas.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total_area, {slice_id: subject_id})\n",
    "    \"\"\"\n",
    "    df = df.dropna()  # Ensure no NaNs\n",
    "    subjects = df.index.tolist()\n",
    "    slices = df.columns.tolist()\n",
    "\n",
    "    # Define problem\n",
    "    prob = pulp.LpProblem(\"Minimize_Total_Mask_Area\", pulp.LpMinimize)\n",
    "\n",
    "    # Define binary decision variables: x[s, sl] = 1 if subject s is chosen for slice sl\n",
    "    x = pulp.LpVariable.dicts(\"x\", ((s, sl) for s in subjects for sl in slices), cat='Binary')\n",
    "\n",
    "    # Objective: minimize total area\n",
    "    prob += pulp.lpSum(x[s, sl] * df.at[s, sl] for s in subjects for sl in slices)\n",
    "\n",
    "    # Constraint: exactly one subject selected per slice\n",
    "    for sl in slices:\n",
    "        prob += pulp.lpSum(x[s, sl] for s in subjects) == 1\n",
    "\n",
    "    # Constraint: each subject can only be used once (across all slices)\n",
    "    for s in subjects:\n",
    "        prob += pulp.lpSum(x[s, sl] for sl in slices) <= 1\n",
    "\n",
    "    # Solve\n",
    "    solver = pulp.PULP_CBC_CMD(msg=False)\n",
    "    prob.solve(solver)\n",
    "\n",
    "    # Extract solution\n",
    "    selected = {sl: s for s in subjects for sl in slices if pulp.value(x[s, sl]) == 1}\n",
    "    total_area = sum(df.at[s, sl] for sl, s in selected.items())\n",
    "\n",
    "    return total_area, selected\n",
    "\n",
    "def copy_selected_masks(df, selected, dataset_name, src_dir, dst_dir):\n",
    "    \"\"\"\n",
    "    Copy selected mask files based on the subject-slice mapping into a new folder.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Area DataFrame (subject × slice).\n",
    "        selected (dict): Mapping {slice_id: subject_id}.\n",
    "        dataset_name (str): One of ['VH', 'WMH2017', 'SHIFTS'].\n",
    "        src_dir (str): Path to the folder containing original mask .png files.\n",
    "        dst_dir (str): Destination directory for selected masks.\n",
    "    \"\"\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    for slice_id, subject_id in selected.items():\n",
    "        filename = None\n",
    "\n",
    "        if dataset_name in ['VH', 'WMH2017']:\n",
    "            # Reconstruct filename: e.g., VH_648_5.png\n",
    "            filename = f\"{dataset_name}_{subject_id}_{slice_id}.png\"\n",
    "\n",
    "        elif dataset_name == 'SHIFTS':\n",
    "            # Subject IDs are strings like 'train14' or 'eval_in2512'\n",
    "            if subject_id.startswith('train_'):\n",
    "                subject_num = subject_id.replace('train_', '')\n",
    "                filename = f\"train_{subject_num}_{slice_id}.png\"\n",
    "            elif subject_id.startswith('dev_in_'):\n",
    "                nums = subject_id.replace('dev_in_', '')\n",
    "                filename = f\"dev_in_{nums}_{slice_id}.png\"\n",
    "            elif subject_id.startswith('eval_in_'):\n",
    "                nums = subject_id.replace('eval_in_', '')\n",
    "                filename = f\"eval_in_{nums}_{slice_id}.png\"\n",
    "            elif subject_id.startswith('dev_out_'):\n",
    "                nums = subject_id.replace('dev_out_', '')\n",
    "                filename = f\"dev_out_{nums}_{slice_id}.png\"\n",
    "\n",
    "        if filename is None:\n",
    "            print(f\"Warning: could not reconstruct filename for subject {subject_id}, slice {slice_id}\")\n",
    "            continue\n",
    "\n",
    "        src_path = os.path.join(src_dir, filename)\n",
    "        dst_path = os.path.join(dst_dir, filename)\n",
    "\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"File not found: {src_path}\")\n",
    "\n",
    "def segment_flair_folder(dst_dir_flair):\n",
    "    # Output folders in ./test_images\n",
    "    base_dir = os.path.dirname(dst_dir_flair.rstrip(\"/\"))\n",
    "    bkgnd_dir = os.path.join(base_dir, \"BKGND\")\n",
    "    csf_dir = os.path.join(base_dir, \"CSF\")\n",
    "    # wm_dir = os.path.join(base_dir, \"WM\")\n",
    "    # gm_dir = os.path.join(base_dir, \"GM\")\n",
    "    wm_gm_dir = os.path.join(base_dir, \"WM_GM\")\n",
    "    os.makedirs(bkgnd_dir, exist_ok=True)\n",
    "    os.makedirs(csf_dir, exist_ok=True)\n",
    "    # os.makedirs(wm_dir, exist_ok=True)\n",
    "    # os.makedirs(gm_dir, exist_ok=True)\n",
    "    os.makedirs(wm_gm_dir, exist_ok=True)\n",
    "\n",
    "    png_files = [f for f in os.listdir(dst_dir_flair) if f.endswith(\".png\") and os.path.isfile(os.path.join(dst_dir_flair, f))]\n",
    "\n",
    "    for filename in png_files:\n",
    "        path = os.path.join(dst_dir_flair, filename)\n",
    "        arr = np.array(Image.open(path).convert(\"L\")).astype(np.float32)\n",
    "\n",
    "        # Create ANTs image\n",
    "        img = ants.from_numpy(arr)\n",
    "        mask = ants.get_mask(img)\n",
    "\n",
    "        # Run Atropos\n",
    "        seg = ants.atropos(\n",
    "            a=img,\n",
    "            m='[0.2,1x1]',\n",
    "            c='[2,0]',\n",
    "            i='kmeans[3]',\n",
    "            x=mask,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Get segmentation labels\n",
    "        seg_image = seg[\"segmentation\"]\n",
    "        unique_labels = np.unique(seg_image.numpy())\n",
    "        means = [img.numpy()[seg_image.numpy() == l].mean() for l in unique_labels]\n",
    "        sorted_labels = np.argsort(means)\n",
    "\n",
    "        # Assign labels based on sorted means\n",
    "        bkgnd_label = unique_labels[sorted_labels[0]]\n",
    "        csf_label = unique_labels[sorted_labels[1]]\n",
    "        wm_label  = unique_labels[sorted_labels[2]]\n",
    "        gm_label  = unique_labels[sorted_labels[3]]\n",
    "\n",
    "        # Create masks\n",
    "        bkgnd_mask = (seg_image == bkgnd_label).numpy().astype(np.uint8) * 255\n",
    "        csf_mask = (seg_image == csf_label).numpy().astype(np.uint8) * 255\n",
    "        # wm_mask = (seg_image == wm_label).numpy().astype(np.uint8) * 255\n",
    "        # gm_mask = (seg_image == gm_label).numpy().astype(np.uint8) * 255\n",
    "        wm_gm_mask = (seg_image == wm_label).numpy().astype(np.uint8) * 255 + (seg_image == gm_label).numpy().astype(np.uint8) * 255\n",
    "\n",
    "        # Save masks\n",
    "        Image.fromarray(bkgnd_mask).save(os.path.join(bkgnd_dir, filename))\n",
    "        Image.fromarray(csf_mask).save(os.path.join(csf_dir, filename))\n",
    "        # Image.fromarray(wm_mask).save(os.path.join(wm_dir, filename))\n",
    "        # Image.fromarray(gm_mask).save(os.path.join(gm_dir, filename))\n",
    "        Image.fromarray(wm_gm_mask).save(os.path.join(wm_gm_dir, filename))\n",
    "\n",
    "        print(f\"Segmented and saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40654f2",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93219bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\"\n",
    "\n",
    "df_vh = build_area_table(directory, 'VH')\n",
    "df_wmh = build_area_table(directory, 'WMH2017')\n",
    "df_shifts = build_area_table(directory, 'SHIFTS')\n",
    "\n",
    "# # Save to CSV or display\n",
    "# df_vh.to_csv(\"vh_mask_area_table.csv\")\n",
    "# print(df_vh.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721612dd",
   "metadata": {},
   "source": [
    "#### 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031e3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum total area: 67\n",
      "Selected subjects per slice:\n",
      "Slice 0: Subject 648\n",
      "Slice 1: Subject 741\n",
      "Slice 2: Subject 729\n",
      "Slice 3: Subject 739\n",
      "Slice 4: Subject 745\n",
      "Slice 5: Subject 746\n",
      "Slice 6: Subject 749\n",
      "Slice 7: Subject 738\n",
      "Slice 8: Subject 727\n",
      "Slice 9: Subject 751\n",
      "Slice 10: Subject 752\n",
      "Slice 11: Subject 754\n",
      "Slice 12: Subject 758\n"
     ]
    }
   ],
   "source": [
    "total_area_vh, best_combo_vh = solve_min_area_ilp(df_vh)\n",
    "\n",
    "print(\"Minimum total area:\", total_area_vh)\n",
    "print(\"Selected subjects per slice:\")\n",
    "for sl in sorted(best_combo_vh):\n",
    "    print(f\"Slice {sl}: Subject {best_combo_vh[sl]}\")\n",
    "\n",
    "src_dir_mask = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\"\n",
    "src_dir_flair = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/flair\"\n",
    "dst_dir_mask = \"./test_images/mask\"\n",
    "dst_dir_flair = \"./test_images/flair\"\n",
    "copy_selected_masks(df_vh, best_combo_vh, 'VH', src_dir_mask, dst_dir_mask)\n",
    "copy_selected_masks(df_vh, best_combo_vh, 'VH', src_dir_flair, dst_dir_flair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fda4099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum total area: 3803\n",
      "Selected subjects per slice:\n",
      "Slice 0: Subject 56\n",
      "Slice 1: Subject 50\n",
      "Slice 2: Subject 132\n",
      "Slice 3: Subject 65\n",
      "Slice 4: Subject 101\n",
      "Slice 5: Subject 59\n",
      "Slice 6: Subject 103\n",
      "Slice 7: Subject 58\n",
      "Slice 8: Subject 137\n",
      "Slice 9: Subject 27\n",
      "Slice 10: Subject 33\n",
      "Slice 11: Subject 6\n",
      "Slice 12: Subject 126\n"
     ]
    }
   ],
   "source": [
    "total_area_wmh, best_combo_wmh = solve_min_area_ilp(df_wmh)\n",
    "\n",
    "print(\"Minimum total area:\", total_area_wmh)\n",
    "print(\"Selected subjects per slice:\")\n",
    "for sl in sorted(best_combo_wmh):\n",
    "    print(f\"Slice {sl}: Subject {best_combo_wmh[sl]}\")\n",
    "\n",
    "src_dir_mask = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\"\n",
    "src_dir_flair = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/flair\"\n",
    "dst_dir_mask = \"./test_images/mask\"\n",
    "dst_dir_flair = \"./test_images/flair\"\n",
    "copy_selected_masks(df_wmh, best_combo_wmh, 'WMH2017', src_dir_mask, dst_dir_mask)\n",
    "copy_selected_masks(df_wmh, best_combo_wmh, 'WMH2017', src_dir_flair, dst_dir_flair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae07562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum total area: 299\n",
      "Selected subjects per slice:\n",
      "Slice 0: Subject train_22\n",
      "Slice 1: Subject dev_in_4\n",
      "Slice 2: Subject train_6\n",
      "Slice 3: Subject eval_in_18\n",
      "Slice 4: Subject dev_out_24\n",
      "Slice 5: Subject train_14\n",
      "Slice 6: Subject train_32\n",
      "Slice 7: Subject dev_out_22\n",
      "Slice 8: Subject dev_out_25\n",
      "Slice 9: Subject eval_in_4\n",
      "Slice 10: Subject eval_in_25\n",
      "Slice 11: Subject train_4\n",
      "Slice 12: Subject eval_in_30\n"
     ]
    }
   ],
   "source": [
    "total_area_shifts, best_combo_shifts = solve_min_area_ilp(df_shifts)\n",
    "\n",
    "print(\"Minimum total area:\", total_area_shifts)\n",
    "print(\"Selected subjects per slice:\")\n",
    "for sl in sorted(best_combo_shifts):\n",
    "    print(f\"Slice {sl}: Subject {best_combo_shifts[sl]}\")\n",
    "\n",
    "src_dir_mask = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\"\n",
    "src_dir_flair = \"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/flair\"\n",
    "dst_dir_mask = \"./test_images/mask\"\n",
    "dst_dir_flair = \"./test_images/flair\"\n",
    "copy_selected_masks(df_shifts, best_combo_shifts, 'SHIFTS', src_dir_mask, dst_dir_mask)\n",
    "copy_selected_masks(df_shifts, best_combo_shifts, 'SHIFTS', src_dir_flair, dst_dir_flair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963d33e",
   "metadata": {},
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18e01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented and saved: WMH2017_27_9.png\n",
      "Segmented and saved: eval_in_30_12.png\n",
      "Segmented and saved: dev_out_22_7.png\n",
      "Segmented and saved: dev_out_24_4.png\n",
      "Segmented and saved: VH_749_6.png\n",
      "Segmented and saved: VH_741_1.png\n",
      "Segmented and saved: train_14_5.png\n",
      "Segmented and saved: eval_in_18_3.png\n",
      "Segmented and saved: VH_746_5.png\n",
      "Segmented and saved: VH_752_10.png\n",
      "Segmented and saved: VH_727_8.png\n",
      "Segmented and saved: VH_754_11.png\n",
      "Segmented and saved: WMH2017_132_2.png\n",
      "Segmented and saved: WMH2017_58_7.png\n",
      "Segmented and saved: WMH2017_103_6.png\n",
      "Segmented and saved: dev_out_25_8.png\n",
      "Segmented and saved: eval_in_4_9.png\n",
      "Segmented and saved: train_22_0.png\n",
      "Segmented and saved: VH_758_12.png\n",
      "Segmented and saved: train_4_11.png\n",
      "Segmented and saved: WMH2017_56_0.png\n",
      "Segmented and saved: train_32_6.png\n",
      "Segmented and saved: WMH2017_50_1.png\n",
      "Segmented and saved: train_6_2.png\n",
      "Segmented and saved: dev_in_4_1.png\n",
      "Segmented and saved: WMH2017_59_5.png\n",
      "Segmented and saved: WMH2017_6_11.png\n",
      "Segmented and saved: WMH2017_101_4.png\n",
      "Segmented and saved: WMH2017_137_8.png\n",
      "Segmented and saved: WMH2017_65_3.png\n",
      "Segmented and saved: WMH2017_33_10.png\n",
      "Segmented and saved: VH_751_9.png\n",
      "Segmented and saved: WMH2017_126_12.png\n",
      "Segmented and saved: VH_729_2.png\n",
      "Segmented and saved: VH_745_4.png\n",
      "Segmented and saved: VH_738_7.png\n",
      "Segmented and saved: eval_in_25_10.png\n",
      "Segmented and saved: VH_648_0.png\n",
      "Segmented and saved: VH_739_3.png\n"
     ]
    }
   ],
   "source": [
    "segment_flair_folder(\"./test_images/flair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf420045",
   "metadata": {},
   "source": [
    "#### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e98b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after reset\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import label\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_lesions(mask):\n",
    "    labeled, n = label(mask)\n",
    "    lesions = []\n",
    "    for i in range(1, n + 1):\n",
    "        lesion = (labeled == i).astype(np.uint8) * 255\n",
    "        area = np.sum(lesion > 0)\n",
    "        lesions.append((lesion, area))\n",
    "    return lesions\n",
    "\n",
    "def categorize_lesions(lesions):\n",
    "    areas = [a for _, a in lesions]\n",
    "    if len(areas) < 3:\n",
    "        return {'small': [], 'medium': [], 'large': []}\n",
    "\n",
    "    small_th = np.percentile(areas, 33)\n",
    "    large_th = np.percentile(areas, 75)\n",
    "\n",
    "    categorized = {'small': [], 'medium': [], 'large': []}\n",
    "    for lesion, area in lesions:\n",
    "        if area <= small_th:\n",
    "            categorized['small'].append(lesion)\n",
    "        elif area <= large_th:\n",
    "            categorized['medium'].append(lesion)\n",
    "        else:\n",
    "            categorized['large'].append(lesion)\n",
    "\n",
    "    return categorized\n",
    "\n",
    "def transform_lesion(lesion, target_shape):\n",
    "    h, w = lesion.shape\n",
    "    scale = random.uniform(0.8, 1.2)\n",
    "    angle = random.uniform(-20, 20)\n",
    "    lesion_resized = cv2.resize(lesion, None, fx=scale, fy=scale)\n",
    "    M = cv2.getRotationMatrix2D((lesion_resized.shape[1] // 2, lesion_resized.shape[0] // 2), angle, 1)\n",
    "    lesion_rotated = cv2.warpAffine(lesion_resized, M, (lesion_resized.shape[1], lesion_resized.shape[0]))\n",
    "    lh, lw = lesion_rotated.shape\n",
    "    if lh >= target_shape[0] or lw >= target_shape[1]:\n",
    "        return None\n",
    "    return lesion_rotated\n",
    "\n",
    "def paste_lesions(target_shape, lesion_dict, wm_mask, num_lesions, existing_mask=None, seed=None):\n",
    "    \"\"\"\n",
    "    Paste specified numbers of small, medium, and large lesions into a blank mask within WM/GM,\n",
    "    avoiding overlap with existing lesion mask if provided.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    new_mask = np.zeros(target_shape, dtype=np.uint8)\n",
    "\n",
    "    for cat in ['small', 'medium', 'large']:\n",
    "        count = num_lesions.get(cat, 0)\n",
    "        if not lesion_dict[cat]:\n",
    "            print(f\"No lesions available for category {cat}\")\n",
    "            continue\n",
    "\n",
    "        placed = 0\n",
    "        tries = 0\n",
    "        while placed < count and tries < 100:\n",
    "            lesion = random.choice(lesion_dict[cat])\n",
    "            lesion_transformed = transform_lesion(lesion, target_shape)\n",
    "            if lesion_transformed is None:\n",
    "                tries += 1\n",
    "                continue\n",
    "\n",
    "            lh, lw = lesion_transformed.shape\n",
    "            x_offset = random.randint(0, target_shape[0] - lh)\n",
    "            y_offset = random.randint(0, target_shape[1] - lw)\n",
    "\n",
    "            lesion_bin = (lesion_transformed > 0).astype(np.uint8)\n",
    "            region = wm_mask[x_offset:x_offset+lh, y_offset:y_offset+lw]\n",
    "            region_bin = (region > 0).astype(np.uint8)\n",
    "\n",
    "            if not np.all(region_bin[lesion_bin == 1] == 1):\n",
    "                tries += 1\n",
    "                continue\n",
    "\n",
    "            # Check overlap with existing lesions if mask is provided\n",
    "            if existing_mask is not None:\n",
    "                existing_region = existing_mask[x_offset:x_offset+lh, y_offset:y_offset+lw]\n",
    "                if np.any((lesion_bin == 1) & (existing_region > 0)):\n",
    "                    tries += 1\n",
    "                    continue\n",
    "\n",
    "            new_mask[x_offset:x_offset+lh, y_offset:y_offset+lw] = np.maximum(\n",
    "                new_mask[x_offset:x_offset+lh, y_offset:y_offset+lw], lesion_bin * 255)\n",
    "            placed += 1\n",
    "\n",
    "    new_mask = (new_mask > 0).astype(np.uint8) * 255\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "def generate_synthetic_mask(source_mask_dir, slice_id, dataset, wm_mask_path, output_path,\n",
    "                            seed=42, num_lesions={'small': 1, 'medium': 1, 'large': 1},\n",
    "                            existing_mask_path=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic lesion mask for a given slice using other subjects' lesions,\n",
    "    avoiding overlap with an existing lesion mask if provided.\n",
    "    \"\"\"\n",
    "    lesion_bank = []\n",
    "    for fname in os.listdir(source_mask_dir):\n",
    "        if not fname.endswith(\".png\"):\n",
    "            continue\n",
    "        if dataset in fname and f\"_{slice_id}.png\" in fname:\n",
    "            mask = np.array(Image.open(os.path.join(source_mask_dir, fname)).convert(\"L\"))\n",
    "            lesions = extract_lesions(mask)\n",
    "            lesion_bank.extend(lesions)\n",
    "\n",
    "    if len(lesion_bank) < sum(num_lesions.values()):\n",
    "        print(f\"Not enough lesions for slice {slice_id} in {dataset}\")\n",
    "        return\n",
    "\n",
    "    lesion_dict = categorize_lesions(lesion_bank)\n",
    "    if not all(lesion_dict[cat] for cat in num_lesions if num_lesions[cat] > 0):\n",
    "        print(f\"Missing categories for slice {slice_id} in {dataset}\")\n",
    "        return\n",
    "\n",
    "    wm_mask = np.array(Image.open(wm_mask_path).convert(\"L\"))\n",
    "    target_shape = wm_mask.shape\n",
    "    existing_mask = None\n",
    "\n",
    "    if existing_mask_path and os.path.exists(existing_mask_path):\n",
    "        existing_mask = np.array(Image.open(existing_mask_path).convert(\"L\"))\n",
    "\n",
    "    synthetic_mask = paste_lesions(target_shape, lesion_dict, wm_mask, num_lesions,\n",
    "                                   existing_mask=existing_mask, seed=seed)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    Image.fromarray(synthetic_mask).save(output_path)\n",
    "\n",
    "    return synthetic_mask\n",
    "\n",
    "def batch_generate_synthetic_masks_flexible(source_mask_dir, mask_folder, wm_mask_folder, output_folder,\n",
    "                                            seed=17844, num_lesions={'small': 1, 'medium': 1, 'large': 1}):\n",
    "    \"\"\"\n",
    "    Loop through all masks in a folder and generate synthetic masks for each one,\n",
    "    supporting both standard and SHIFTS-style filenames.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(mask_folder):\n",
    "        if not fname.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        dataset, slice_id = None, None\n",
    "\n",
    "        # Pattern 1: Standard (VH_749_6.png, WMH2017_50_1.png)\n",
    "        match_std = re.match(r\"^(VH|WMH2017)_(\\d+)_([0-9]+)\\.png$\", fname)\n",
    "        if match_std:\n",
    "            dataset, subject_id, slice_id = match_std.groups()\n",
    "            slice_id = int(slice_id)\n",
    "\n",
    "        # Pattern 2: SHIFTS format (train_14_5.png, eval_in_30_12.png)\n",
    "        match_shifts = re.match(r\"^(train|dev_in|dev_out|eval_in)_(\\d+)_([0-9]+)\\.png$\", fname)\n",
    "        if match_shifts:\n",
    "            prefix, subject_id, slice_id = match_shifts.groups()\n",
    "            dataset = prefix  # e.g., 'train'\n",
    "            slice_id = int(slice_id)\n",
    "\n",
    "        if dataset is None or slice_id is None:\n",
    "            print(f\"Skipping unrecognized filename format: {fname}\")\n",
    "            continue\n",
    "\n",
    "        # Build paths\n",
    "        existing_mask_path = os.path.join(mask_folder, fname)\n",
    "        wm_mask_path = os.path.join(wm_mask_folder, fname)\n",
    "        output_path = os.path.join(output_folder, fname)\n",
    "\n",
    "        # Generate synthetic mask\n",
    "        print(f\"Generating synthetic mask for {fname}\")\n",
    "        generate_synthetic_mask(\n",
    "            source_mask_dir=source_mask_dir,\n",
    "            slice_id=slice_id,\n",
    "            dataset=dataset,\n",
    "            wm_mask_path=wm_mask_path,\n",
    "            output_path=output_path,\n",
    "            existing_mask_path=existing_mask_path,\n",
    "            seed=seed,\n",
    "            num_lesions=num_lesions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cc65f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic mask for WMH2017_27_9.png\n",
      "Generating synthetic mask for eval_in_30_12.png\n",
      "Generating synthetic mask for dev_out_22_7.png\n",
      "Generating synthetic mask for dev_out_24_4.png\n",
      "Generating synthetic mask for VH_749_6.png\n",
      "Generating synthetic mask for VH_741_1.png\n",
      "Generating synthetic mask for train_14_5.png\n",
      "Generating synthetic mask for eval_in_18_3.png\n",
      "Generating synthetic mask for VH_746_5.png\n",
      "Generating synthetic mask for VH_752_10.png\n",
      "Generating synthetic mask for VH_727_8.png\n",
      "Generating synthetic mask for VH_754_11.png\n",
      "Generating synthetic mask for WMH2017_132_2.png\n",
      "Generating synthetic mask for WMH2017_58_7.png\n",
      "Generating synthetic mask for WMH2017_103_6.png\n",
      "Generating synthetic mask for dev_out_25_8.png\n",
      "Generating synthetic mask for eval_in_4_9.png\n",
      "Generating synthetic mask for train_22_0.png\n",
      "Generating synthetic mask for VH_758_12.png\n",
      "Generating synthetic mask for train_4_11.png\n",
      "Generating synthetic mask for WMH2017_56_0.png\n",
      "Generating synthetic mask for train_32_6.png\n",
      "Generating synthetic mask for WMH2017_50_1.png\n",
      "Generating synthetic mask for train_6_2.png\n",
      "Generating synthetic mask for dev_in_4_1.png\n",
      "Generating synthetic mask for WMH2017_59_5.png\n",
      "Generating synthetic mask for WMH2017_6_11.png\n",
      "Generating synthetic mask for WMH2017_101_4.png\n",
      "Generating synthetic mask for WMH2017_137_8.png\n",
      "Generating synthetic mask for WMH2017_65_3.png\n",
      "Generating synthetic mask for WMH2017_33_10.png\n",
      "Generating synthetic mask for VH_751_9.png\n",
      "Generating synthetic mask for WMH2017_126_12.png\n",
      "Generating synthetic mask for VH_729_2.png\n",
      "Generating synthetic mask for VH_745_4.png\n",
      "Generating synthetic mask for VH_738_7.png\n",
      "Generating synthetic mask for eval_in_25_10.png\n",
      "Generating synthetic mask for VH_648_0.png\n",
      "Generating synthetic mask for VH_739_3.png\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_generate_synthetic_masks_flexible(\n",
    "    source_mask_dir=\"/home/benet/data/lesion2D_VH-SHIFTS-WMH2017_empty_masks/test/mask\",\n",
    "    mask_folder=\"./test_images/mask\",\n",
    "    wm_mask_folder=\"./test_images/WM_GM\",\n",
    "    output_folder=\"./test_images/synthetic_masks_big\",\n",
    "    seed=17844,\n",
    "    num_lesions={'small': 0, 'medium': 0, 'large': 3}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benetvicorob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
