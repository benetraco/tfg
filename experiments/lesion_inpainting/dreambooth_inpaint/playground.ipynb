{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to try implementations/functions of the dreambooth inpainting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device: 0\n",
      "Device name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Automatically add the project root (tfg/) to the Python path \"../../..\"\n",
    "project_root = \"/home/benet/tfg\"\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Restrict PyTorch to use only GPU X\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import yaml\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    DiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    StableDiffusionInpaintPipeline,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, replace_example_docstring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import the custom pipeline with histogram support\n",
    "from experiments.lesion_inpainting.dreambooth_inpaint.hist_control.histogram_control import HistogramInpaintPipeline, HistogramConditioning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"CUDA device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n",
    "check_min_version(\"0.13.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "  \n",
    "def mse_lesion_loss(image, target, mask):\n",
    "    \"\"\"Compute the MSE loss between the image and target, only considering the lesion area.\"\"\"\n",
    "    return F.mse_loss(image * mask, target * mask)\n",
    "\n",
    "# Inplementation of validation logging with multiple images from the validation dataset\n",
    "def log_validation_dataset(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, global_step, val_dataset, hist_conditioning=None):\n",
    "    \"\"\"Validates the model using multiple input images from the validation dataset and logs the results.\"\"\"\n",
    "    logger.info(f\"Running validation... \\n Generating {args.num_validation_images} images per input.\")\n",
    "\n",
    "    # Load your custom pipeline with histogram support\n",
    "    pipeline = HistogramInpaintPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        tokenizer=tokenizer,\n",
    "        unet=accelerator.unwrap_model(unet),\n",
    "        vae=vae,\n",
    "        hist_conditioning=accelerator.unwrap_model(hist_conditioning),\n",
    "        torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "\n",
    "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "\n",
    "    total_mse_losses, all_logs = [], []\n",
    "\n",
    "    # Load histogram bank\n",
    "    histogram_bank = torch.load(os.path.join(args.output_dir, \"histogram_bank.pt\")).to(accelerator.device)\n",
    "    num_clusters = histogram_bank.shape[0]\n",
    "    total_mse_losses_cluster = [[] for _ in range(num_clusters)]\n",
    "\n",
    "    log_pbar = tqdm(total=len(val_dataset), desc=\"Validation\", position=0, leave=True)\n",
    "    for j in range(len(val_dataset)):\n",
    "\n",
    "        # Get the preprocessed tensors for loss\n",
    "        input_image_tensor = val_dataset[j][\"instance_images\"].to(accelerator.device)\n",
    "        mask_tensor = val_dataset[j][\"lesion_masks\"].to(accelerator.device)\n",
    "\n",
    "        # Use the original unnormalized images for logging and pipeline\n",
    "        input_image_pil = val_dataset[j][\"PIL_images\"]\n",
    "        mask_pil = transforms.ToPILImage()(mask_tensor.cpu())  # Convert mask to PIL (if not already)\n",
    "\n",
    "        images, losses = [], [[] for _ in range(num_clusters)]\n",
    "\n",
    "        for cluster_idx in range(num_clusters):\n",
    "            hist = histogram_bank[cluster_idx].unsqueeze(0)  # shape (1, 32)\n",
    "\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                image = pipeline(\n",
    "                    args.validation_prompt,\n",
    "                    image=input_image_pil,\n",
    "                    mask_image=mask_pil,\n",
    "                    hist=hist,\n",
    "                    num_inference_steps=25,\n",
    "                ).images[0]\n",
    "                images.append(image)\n",
    "\n",
    "                # Compute masked MSE\n",
    "                image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(accelerator.device)\n",
    "                mse_loss = mse_lesion_loss(image_tensor, input_image_tensor.unsqueeze(0), mask_tensor.unsqueeze(0)).item()\n",
    "                losses[cluster_idx].append(mse_loss)\n",
    "\n",
    "        avg_mse_loss = sum(losses) / len(losses)\n",
    "        total_mse_losses.append(avg_mse_loss)\n",
    "        for i, loss in enumerate(losses):\n",
    "            total_mse_losses_cluster[i].append(loss)\n",
    "\n",
    "        if j < args.num_validation_images_to_log:\n",
    "            logs = {\n",
    "                f\"Validation {j}\": [\n",
    "                    wandb.Image(input_image_pil, caption=\"Input\"),\n",
    "                    wandb.Image(mask_pil, caption=\"Mask\")\n",
    "                ] + [wandb.Image(img, caption=f\"Cluster {i}\") for i, img in enumerate(images)],\n",
    "            }\n",
    "            all_logs.append(logs)\n",
    "\n",
    "        log_pbar.update(1)\n",
    "        log_pbar.set_postfix({\"Validation loss\": avg_mse_loss})\n",
    "    \n",
    "    log_pbar.close()\n",
    "    \n",
    "    final_mse_loss = sum(total_mse_losses) / len(total_mse_losses)\n",
    "    for i, cluster_losses in enumerate(total_mse_losses_cluster):\n",
    "        cluster_loss = sum(cluster_losses) / len(cluster_losses)\n",
    "        all_logs.append({f\"Validation loss hist cluster {i}\": cluster_loss})\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        if tracker.name == \"wandb\":\n",
    "            for logs in all_logs:\n",
    "                tracker.log(logs, step=global_step)\n",
    "            tracker.log({\"Validation loss\": final_mse_loss,}, step = global_step)\n",
    "        else:\n",
    "            raise ValueError(f\"Tracker '{accelerator.tracker.name}' is not supported for validation logging.\")\n",
    "\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def compute_masked_histogram(image, mask, num_bins=32):\n",
    "    # Normalize image to [0, 1]\n",
    "    image = (image + 1.0) / 2.0  # from [-1, 1] to [0, 1]\n",
    "    mask = mask.squeeze(0)  # shape [512, 512]\n",
    "    masked_pixels = image[0][mask > 0]  # use only the first channel\n",
    "    if masked_pixels.numel() == 0:\n",
    "        hist = torch.zeros(num_bins)\n",
    "    else:\n",
    "        hist = torch.histc(masked_pixels, bins=num_bins, min=0.0, max=1.0)\n",
    "        hist = hist / hist.sum()  # Normalize to sum to 1\n",
    "    return hist\n",
    "\n",
    "def prepare_mask_and_masked_image(image, mask, black_mask=True, discretize_mask=True):\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    # image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "\n",
    "    mask = np.array(mask.convert(\"L\"))\n",
    "    mask = mask.astype(np.float32) / 255.0\n",
    "    # mask = mask[None, None]\n",
    "    mask = mask[None]\n",
    "    if discretize_mask:\n",
    "        mask[mask < 0.5] = 0\n",
    "        mask[mask >= 0.5] = 1\n",
    "    else:\n",
    "        mask[mask < 0.0] = 0\n",
    "        mask[mask >= 1.0] = 1\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    if black_mask:\n",
    "        masked_image = image * (mask < 0.5) + (mask >= 0.5) * -1 if discretize_mask else image * (1 - mask)\n",
    "    else:\n",
    "        masked_image = image * (mask < 0.5)\n",
    "\n",
    "    return mask, masked_image\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    config_path = \"config_dreambooth_inpaint.yaml\"\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file '{config_path}' not found.\")\n",
    "    \n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Convert values in config to the correct types (float or int)\n",
    "    def convert_values(config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            # If the value is a string that can be converted to float or int, convert it\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Try converting to float first (for cases like '5e-6')\n",
    "                    config_dict[key] = float(value) if '.' in value or 'e' in value else int(value)\n",
    "                except ValueError:\n",
    "                    # If conversion fails, leave as string\n",
    "                    pass\n",
    "            # Recursively process dicts (in case there are nested structures)\n",
    "            elif isinstance(value, dict):\n",
    "                convert_values(value)\n",
    "\n",
    "    # Convert all values\n",
    "    convert_values(config)\n",
    "\n",
    "    class Args:\n",
    "        def __init__(self, config_dict):\n",
    "            for key, value in config_dict.items():\n",
    "                setattr(self, key, value)\n",
    "    \n",
    "    args = Args(config)\n",
    "    \n",
    "    # Ensure local_rank consistency with environment variable\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if hasattr(args, \"local_rank\") and env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "    \n",
    "    # Validate required arguments\n",
    "    if not hasattr(args, \"instance_data_dir\") or args.instance_data_dir is None:\n",
    "        raise ValueError(\"You must specify a train data directory.\")\n",
    "        \n",
    "    return args\n",
    "\n",
    "\n",
    "class MSInpaintingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for MS lesion inpainting. Loads paired FLAIR MRI images and corresponding lesion masks.\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths, # List of image file paths\n",
    "        mask_paths, # List of corresponding mask file paths\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        size=512,\n",
    "        black_mask=True,\n",
    "        discretize_mask=True,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.black_mask = black_mask\n",
    "        self.discretize_mask = discretize_mask\n",
    "\n",
    "        # self.instance_data_root = Path(instance_data_root)\n",
    "        # self.mask_data_root = Path(mask_data_root)\n",
    "        # if not self.instance_data_root.exists() or not self.mask_data_root.exists():\n",
    "        #     raise ValueError(\"Instance images root or mask images root doesn't exists.\")\n",
    "\n",
    "        # self.image_paths = sorted(list(self.instance_data_root.iterdir()))\n",
    "        # self.mask_paths = sorted([self.mask_data_root / img.name for img in self.image_paths]) # Corresponding masks for each image\n",
    "        \n",
    "        self.image_paths = image_paths  # List of image file paths\n",
    "        self.mask_paths = mask_paths  # List of corresponding mask file paths\n",
    "\n",
    "\n",
    "        # Ensure there are corresponding masks for each image\n",
    "        assert all(mask.exists() for mask in self.mask_paths), \"Some masks are missing for the images!\"\n",
    "\n",
    "        self.num_instance_images = len(self.image_paths)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        self.image_transforms_resize_and_crop = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image_path = self.image_paths[index % self.num_instance_images]\n",
    "        instance_image = Image.open(instance_image_path)\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        instance_image = self.image_transforms_resize_and_crop(instance_image)\n",
    "        instance_prompt = self.instance_prompt\n",
    "        mask = Image.open(self.mask_paths[index])\n",
    "        if not mask.mode == \"L\":\n",
    "            mask = mask.convert(\"L\")\n",
    "        mask = self.image_transforms_resize_and_crop(mask)\n",
    "        # prepare mask and masked image\n",
    "        mask, masked_image = prepare_mask_and_masked_image(instance_image, mask, black_mask=self.black_mask, discretize_mask=self.discretize_mask)\n",
    "\n",
    "        example[\"lesion_masks\"] = mask\n",
    "        example[\"masked_images\"] = masked_image\n",
    "        example[\"PIL_images\"] = instance_image\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            instance_prompt,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "        example[\"histogram\"] = compute_masked_histogram(example[\"instance_images\"], mask)\n",
    "\n",
    "        return example\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    \"\"\"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\"\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/27/2025 14:23:35 - INFO - __main__ -   Using cuda device\n",
      "An error occurred while trying to fetch stable-diffusion-v1-5/stable-diffusion-inpainting: stable-diffusion-v1-5/stable-diffusion-inpainting does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch stable-diffusion-v1-5/stable-diffusion-inpainting: stable-diffusion-v1-5/stable-diffusion-inpainting does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "03/27/2025 14:23:39 - INFO - __main__ -   Models loaded successfully\n",
      "03/27/2025 14:23:39 - INFO - __main__ -   Optimizer and scheduler created\n",
      "03/27/2025 14:23:40 - INFO - __main__ -   Data loaded successfully. Length of dataset: 513. Length of dataloader: 129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenetraco\u001b[0m (\u001b[33mbenetraco-universitat-polit-cnica-de-catalunya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/benet/tfg/experiments/lesion_inpainting/dreambooth_inpaint/wandb/run-20250327_142341-yjomke9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth/runs/yjomke9b' target=\"_blank\">glad-armadillo-56</a></strong> to <a href='https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth' target=\"_blank\">https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth/runs/yjomke9b' target=\"_blank\">https://wandb.ai/benetraco-universitat-polit-cnica-de-catalunya/dreambooth/runs/yjomke9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "project_config = ProjectConfiguration(\n",
    "    total_limit=args.checkpoints_total_limit, project_dir=args.output_dir, logging_dir=logging_dir\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.log_with,\n",
    "    project_config=project_config,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(f\"Using {accelerator.device.type} device\")\n",
    "\n",
    "# Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n",
    "# This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n",
    "# TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n",
    "if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n",
    "    raise ValueError(\n",
    "        \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n",
    "        \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n",
    "    )\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        repo_id = create_repo(\n",
    "            repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n",
    "        ).repo_id\n",
    "\n",
    "# Load the tokenizer\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n",
    "elif args.pretrained_model_name_or_path:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "\n",
    "# Load models and create wrapper for stable diffusion\n",
    "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "logger.info(\"Models loaded successfully\")\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "if not args.train_text_encoder:\n",
    "    text_encoder.requires_grad_(False)\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    if args.train_text_encoder:\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "if args.scale_lr:\n",
    "    args.learning_rate = (\n",
    "        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "    )\n",
    "\n",
    "# Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "if args.use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "        )\n",
    "\n",
    "    optimizer_class = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_class = torch.optim.AdamW\n",
    "\n",
    "params_to_optimize = (\n",
    "    itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    params_to_optimize,\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "logger.info(\"Optimizer and scheduler created\")\n",
    "\n",
    "# Load dataset\n",
    "train_image_paths = sorted(list(Path(args.instance_data_dir).iterdir()))\n",
    "train_mask_paths = sorted([Path(args.mask_data_dir) / img.name for img in train_image_paths])\n",
    "\n",
    "# Training dataset\n",
    "full_dataset = MSInpaintingDataset(\n",
    "    image_paths=train_image_paths,\n",
    "    mask_paths=train_mask_paths,\n",
    "    instance_prompt=args.instance_prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    size=args.resolution,\n",
    "    black_mask=args.black_mask,\n",
    "    discretize_mask=args.discretize_mask,\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(full_dataset))), test_size=args.validation_split, random_state=args.seed)\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    masks = [example[\"lesion_masks\"] for example in examples]\n",
    "    masked_images = [example[\"masked_images\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()\n",
    "    masks = torch.stack(masks)\n",
    "    masked_images = torch.stack(masked_images)\n",
    "\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    hists = [example[\"histogram\"] for example in examples]\n",
    "    histograms = torch.stack(hists)\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"masks\": masks,\n",
    "        \"masked_images\": masked_images,\n",
    "        \"histograms\": histograms,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "logger.info(f\"Data loaded successfully. Length of dataset: {len(train_dataset)}. Length of dataloader: {len(train_dataloader)}\")\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    ")\n",
    "\n",
    "if args.train_text_encoder:\n",
    "    unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "else:\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "accelerator.register_for_checkpointing(lr_scheduler)\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if args.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif args.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move text_encode and vae to gpu.\n",
    "# For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "# as these models are only used for inference, keeping weights in full precision is not required.\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "if not args.train_text_encoder:\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(\"dreambooth\", config=vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved histogram bank to: lesion-inpating-dreambooth-model-histcont/histogram_bank.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the histogram conditioning model and save the histogram bank\n",
    "hist_conditioning = HistogramConditioning().to(accelerator.device)\n",
    "hist_conditioning.train()\n",
    "hist_conditioning = accelerator.prepare(hist_conditioning)\n",
    "all_histograms = torch.stack([example[\"histogram\"] for example in train_dataset])\n",
    "kmeans = KMeans(n_clusters=3, random_state=17844).fit(all_histograms.numpy())\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "torch.save(torch.tensor(cluster_centers), os.path.join(args.output_dir, \"histogram_bank.pt\"))\n",
    "print(\"Saved histogram bank to:\", os.path.join(args.output_dir, \"histogram_bank.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 570, Masks: 570\n",
      "[PosixPath('/home/benet/data/VH2D/images/all/flair/600_0.png'), PosixPath('/home/benet/data/VH2D/images/all/flair/600_1.png'), PosixPath('/home/benet/data/VH2D/images/all/flair/600_2.png'), PosixPath('/home/benet/data/VH2D/images/all/flair/600_3.png'), PosixPath('/home/benet/data/VH2D/images/all/flair/600_4.png')]\n",
      "[PosixPath('/home/benet/data/VH2D/images/all/mask/600_0.png'), PosixPath('/home/benet/data/VH2D/images/all/mask/600_1.png'), PosixPath('/home/benet/data/VH2D/images/all/mask/600_2.png'), PosixPath('/home/benet/data/VH2D/images/all/mask/600_3.png'), PosixPath('/home/benet/data/VH2D/images/all/mask/600_4.png')]\n",
      "Train dataset length: 513\n",
      "dict_keys(['lesion_masks', 'masked_images', 'PIL_images', 'instance_images', 'instance_prompt_ids', 'histogram'])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Images: {len(train_image_paths)}, Masks: {len(train_mask_paths)}\")\n",
    "print(train_image_paths[:5])\n",
    "print(train_mask_paths[:5])\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(sample.keys())\n",
    "print(sample[\"histogram\"].shape)\n",
    "\n",
    "hist_list = [example[\"histogram\"] for example in train_dataset]\n",
    "if len(hist_list) == 0:\n",
    "    raise RuntimeError(\"No histograms were found. Check your dataset and preprocessing.\")\n",
    "all_histograms = torch.stack(hist_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistogram\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example' is not defined"
     ]
    }
   ],
   "source": [
    "print(example[\"histogram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benetvicorob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
