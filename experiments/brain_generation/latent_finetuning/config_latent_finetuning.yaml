# configuration file for the unconditional training
# tmux: tmux new -s latent_finetuning
# 1. Image processing
processing:
  # dataset name
  # dataset: "/home/benet/data/VH2D/latent_flair" # name of the dataset or directory in repo
  # dataset: "/home/benet/data/VH2D/latent_flair_scaled"
  dataset: "/home/benet/data/generation2D_VH-SHIFTS-WMH2017/latent_flair_scaled"
  # image size
  resolution: 32
  # interpolation: "BILINEAR"
  # # normalisation
  # normalisation_value: 4095.0
  # batch size for dataloader
  batch_size: 8
  # number of workers for dataloader
  num_workers: 4 # 0 means no extra processes are used (run in main process)

# 2. Prompt
# prompt: ""
prompt: "Axial FLAIR MRI scan of the human brain" # Prompt for the model
# prompt: "A high-resolution axial FLAIR MRI scan of the human brain, showing clear white matter hyperintensities, detailed ventricles, and cortical structures. The image is grayscale with well-defined contrast between cerebrospinal fluid, gray matter, and white matter. No artifacts, clear and medically accurate."
# prompt: "Axial T1 MRI scan of the human brain" # Prompt for the model

# 3. Training
training:
  num_epochs: 50 # Number of epochs to train for
  gradient_accumulation:
    steps: 4 # Number of gradient accumulation steps
  mixed_precision:
    type: 'no' # Type of mixed precision to use
  gradient_clip:
    max_norm: 1.0 # Maximum norm for gradient clipping
  enable_xformers_memory_efficient_attention: False
  optimizer:
    learning_rate: 1.0e-4 # Learning rate for the optimizer
    beta_1: 0.95 # Beta 1 for the AdamW optimizer
    beta_2: 0.999 # Beta 2 for the AdamW optimizer
    weight_decay: 1.0e-6
    eps: 1.0e-8
  lr_scheduler:
    name: "cosine"
    num_warmup_steps: 500
  noise_scheduler:
    type: "DDPM" # Type of noise scheduler to use, options: "DDIM" or "DDPM"
    num_train_timesteps: 1000
    beta_schedule: "scaled_linear" #"linear" # originally using "squaredcos_cap_v2", 
    beta_start: 0.0001 # default 0.0001
    beta_end: 0.02 # default 0.02

# 4. Saving and logging
saving:
  local:
    outputs_dir: 'results/pipelines' # Parent directory for saving outputs
    pipeline_name: 'latent_finetuned_prompts' # Name of the pipeline
    checkpoint_frequency: 10000 # How often to save checkpoints (in steps)
    saving_frequency: 10 # How often to save the model (in epochs)
    vae_name : 'fintuned_vae' # Name of the VAE model
  hf:
    # repo_name: 'Latent_Breast' # Name of the HF repo
    # model_card_path: 'model_cards/mc_latent.yaml' # # path from experiment directory
logging:
  logger_name: 'wandb' # Name of the logger
  dir_name: 'logs' # Name of the logging directory
  # run_name: '40k' # Name of the run
  images:
    freq_epochs: 5 # How often to save images (in epochs)
    batch_size: 4 # Batch size for image generation
    scaled: True # Whether to scale the images
  log_reconstructions: True # Whether to log reconstructions
  guidance: True # Whether to log guidance