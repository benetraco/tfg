# configuration file for the unconditional training

# 1. Image processing
processing:
  # dataset name
  dataset: "/home/benet/data/VH2D/images/flair" # name of the dataset or directory in repo
  # image size or resolution to resize
  resolution: 256
  interpolation: "BILINEAR"
  # batch size for dataloader
  batch_size: 2
  # number of workers for dataloader
  num_workers: 12 # 0 means no extra processes are used (run in main process)

# 3. Training
training:
  num_epochs: 20 # Number of epochs to train for
  gradient_accumulation:
    steps: 16 # Number of gradient accumulation steps
  mixed_precision:
    type: 'no' # Type of mixed precision to use
  gradient_clip:
    max_norm: 1.0 # Maximum norm for gradient clipping
  optimizer:
    learning_rate: 1.0e-4 # Learning rate for the optimizer
    beta_1: 0.95 # Beta 1 for the AdamW optimizer
    beta_2: 0.999 # Beta 2 for the AdamW optimizer
    weight_decay: 1.0e-6
    eps: 1.0e-8
  lr_scheduler:
    name: "cosine"
    num_warmup_steps: 0

# 4. Saving and logging
saving:
  local:
    outputs_dir: 'results/pipelines' # Parent directory for saving outputs
    pipeline_name: 'fintuned_vae' # Name of the pipeline
    checkpoint_frequency: 10000 # How often to save checkpoints (in steps)
    saving_frequency: 10 # How often to save the model (in epochs)
logging:
  logger_name: 'wandb' # Name of the logger
  images:
    freq_epochs: 1 # How often to save images (in epochs)
    batch_size: 4 # Batch size for image generation